Nandish Jha (naj474) - Deliverable/Assignment 4




Question 1 (b) - What is optimal value?
    The optimal values for the range of 1-50 are: 0-6, 8-35, 37, 39, 41-43, 45.
    Moreover, the kNN classifier is more accurate than the rNN classifier under the set parameters.
    We can see this by checking their F1 score printed in the Q1.ipynb.
        KNN Classifier Metrics: Accuracy: 1.00, F1 Score: 1.00
        rNN Classifier Metrics: Accuracy: 0.83, F1 Score: 0.83

Does your answer differ depending on the validation strategy used to assess the performance?
    The optimal value may differ depending on the validation strategy used to assess the performance.
    For example, if you use a simple train-test split for validation, the optimal value might be
    different from when using cross-validation. This difference occurs because the data split might
    affect the generalization of the model, leading to variations in the optimal parameter value.
    Cross-validation provides a more robust estimate of the model's performance by averaging over
    multiple train-test splits, potentially resulting in a more stable optimal parameter value.
    Therefore, it's essential to consider the validation strategy when determining the optimal
    parameter value.




Question 2 (a) - Justify your parameter choice and kernel used.
    We generate a synthetic dataset using make_circles.
    We split the data into training and testing sets.
    We design an SVM classifier using the radial basis function (RBF) kernel (kernel='rbf') with
    default parameters. The RBF kernel is suitable for non-linearly separable data like the one 
    generated by make_circles.
    We fit the classifier to the training data and plot the decision boundary.

Question 2 (b) - Comment on your findings.
    By analyzing the resulting plot, we can observe how the classifier's performance changes with
    different amounts of training data. Typically, we expect the testing accuracy to improve as we
    increase the amount of training data, but it may plateau or even decrease in certain cases due to
    overfitting or underfitting.




Question 3 (a) - Justify your final selected design.
    We load the diabetes dataset.
    We split the data into training and testing sets.
    We design a regressor using Linear Regression, a simple and interpretable model.
    We fit the regressor to the training data and make predictions on the testing data.
    We calculate the Mean Squared Error (MSE) as the performance metric to evaluate the regressor.

Question 3 (b) - Comment on your findings.
    By analyzing the resulting plot, we can observe how the regressor's performance changes with
    different amounts of training data. Typically, we expect the MSE to decrease as we increase the
    amount of training data, but it may plateau or even increase in certain cases due to overfitting
    or underfitting.




Question 4 (a) - Explain the meaning of an imbalanced dataset. (etc.)
    An imbalanced dataset refers to a situation where the classes within the dataset are not represented
    equally. This means that one class may have significantly more instances than another. Imbalanced
    datasets can be problematic in machine learning because classifiers trained on such data tend
    to be biased towards the majority class, leading to poor performance on the minority class.
    In the digits dataset, this issue may arise if some digits (e.g., '1' or '0') are represented
    more than others.

Question 4 (b) - Explain the PROs/CONs of the accuracy score vs. the F1-score.
    Accuracy score measures the proportion of correctly classified instances out of the total instances.
    It is a simple and intuitive metric but can be misleading in the presence of imbalanced datasets
    since it doesn't consider the distribution of classes. On the other hand, the F1-score considers
    both precision and recall, making it suitable for imbalanced datasets. It balances between precision
    (the number of true positive predictions divided by the total number of positive predictions) and
    recall (the number of true positive predictions divided by the total number of actual positives),
    providing a better understanding of the classifier's performance across different classes.

Question 4 (c) - Justify your final selected design.
    For the digits dataset, a suitable classifier could be the Support Vector Machine (SVM) classifier.
    SVM is known for its effectiveness in handling high-dimensional data and can perform well even
    with a relatively small dataset like digits. We can use the radial basis function (RBF) kernel
    for non-linear classification. The parameter `C` can be selected using cross-validation to control
    the trade-off between maximizing the margin and minimizing the classification error. Performance
    can be evaluated using the F1-score since it provides a balanced measure of precision and recall,
    which is crucial for imbalanced datasets.

Question 4 (d) - Evaluate the performance using this method, and comment on your results.
    K-fold cross-validation is a technique used to assess the performance of a machine learning model.
    It involves splitting the dataset into k subsets (folds) and training the model k times, each time
    using a different fold as the test set and the remaining folds as the training set. The performance
    is then averaged over all k iterations to obtain a more reliable estimate. The results from K-fold
    cross-validation provide a more robust estimate of the classifier's performance compared to a single
    train-test split. It helps to assess how well the classifier generalizes to unseen data and reduces
    the impact of variability in the train-test split.
